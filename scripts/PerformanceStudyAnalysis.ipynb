{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dataset_properties = {'jester': {'n_folds': 8,\n",
    "                       'evaluation_predicate': 'rating'\n",
    "                                }, \n",
    "                      'epinions': {'n_folds': 8,\n",
    "                                   'evaluation_predicate': 'trusts'\n",
    "                                  },\n",
    "                      'cora': {'n_folds': 8, \n",
    "                               'evaluation_predicate': 'hasCat'\n",
    "                              },\n",
    "                      'citeseer': {'n_folds': 8,\n",
    "                                   'evaluation_predicate': 'hasCat'\n",
    "                                  },\n",
    "                      'lastFM': {'n_folds': 5, \n",
    "                                 'evaluation_predicate': 'rating'\n",
    "                                }\n",
    "                     }\n",
    "\n",
    "# weight_learning_methods = [\"BOWLSS\", \"BOWLOS\", \"LME\", \"MLE\",\n",
    "#            \"MPLE\", \"RGS\", \"CRGS\", \"HB\"]\n",
    "\n",
    "weight_learning_methods = {\"psl\": [\"BOWLSS\", \"BOWLOS\", \"UNIFORM\"],\n",
    "                           \"tuffy\": [\"UNIFORM\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/charles/SRLWeightLearning/scripts\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset=\"epinions\"\n",
    "# wl_method=\"UNIFORM\"\n",
    "# evaluation_metric=\"Discrete\"\n",
    "# fold=0\n",
    "# predicate=\"trusts\"\n",
    "\n",
    "dataset=\"citeseer\"\n",
    "wl_method=\"MLE\"\n",
    "evaluation_metric=\"Categorical\"\n",
    "fold=1\n",
    "predicate=\"hasCat\"\n",
    "\n",
    "# dataset=\"jester\"\n",
    "# wl_method=\"UNIFORM\"\n",
    "# evaluation_metric=\"Continuous\"\n",
    "# fold=0\n",
    "# predicate=\"rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    output = []\n",
    "\n",
    "    with open(filename, 'r') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            output.append(line)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_observed_frame(dataset, fold, predicate, phase='eval'):\n",
    "    # path to this file relative to caller\n",
    "#     dirname = os.path.dirname(__file__)\n",
    "    dirname='.'\n",
    "\n",
    "    observed_path = \"{}/../psl-examples/{}/data/{}/{}/{}/{}_obs.txt\".format(dirname, dataset, dataset, fold, phase, predicate)\n",
    "    observed_df = pd.read_csv(observed_path, sep='\\t', header=None)\n",
    "\n",
    "    # clean up column names and set multi-index for predicate\n",
    "    arg_columns = ['arg_' + str(col) for col in observed_df.columns[:-1]]\n",
    "    value_column = ['val']\n",
    "    observed_df.columns = arg_columns + value_column\n",
    "    observed_df = observed_df.astype({col: int for col in arg_columns})\n",
    "    observed_df = observed_df.set_index(arg_columns)\n",
    "\n",
    "    return observed_df\n",
    "\n",
    "\n",
    "def load_truth_frame(dataset, fold, predicate, phase='eval'):\n",
    "    # path to this file relative to caller\n",
    "#     dirname = os.path.dirname(__file__)\n",
    "    dirname='.'\n",
    "\n",
    "    truth_path = \"{}/../psl-examples/{}/data/{}/{}/{}/{}_truth.txt\".format(dirname, dataset, dataset, fold, phase, predicate)\n",
    "    truth_df = pd.read_csv(truth_path, sep='\\t', header=None)\n",
    "\n",
    "    # clean up column names and set multi-index for predicate\n",
    "    arg_columns = ['arg_' + str(col) for col in truth_df.columns[:-1]]\n",
    "    value_column = ['val']\n",
    "    truth_df.columns = arg_columns + value_column\n",
    "    truth_df = truth_df.astype({col: int for col in arg_columns})\n",
    "    truth_df = truth_df.set_index(arg_columns)\n",
    "\n",
    "    return truth_df\n",
    "\n",
    "\n",
    "def load_target_frame(dataset, fold, predicate, phase='eval'):\n",
    "    # path to this file relative to caller\n",
    "    #     dirname = os.path.dirname(__file__)\n",
    "    dirname='.'\n",
    "\n",
    "    # TODO: (Charles D.) This is a hack, there should be a way to find what\n",
    "    #  the suffix should be for this file from the cli in the psl-examples but its either target or targets\n",
    "    try:\n",
    "        target_path = \"{}/../psl-examples/{}/data/{}/{}/{}/{}_target.txt\".format(dirname, dataset, dataset, fold,\n",
    "                                                                                 phase, predicate)\n",
    "        target_df = pd.read_csv(target_path, sep='\\t', header=None)\n",
    "    except FileNotFoundError as err:\n",
    "        target_path = \"{}/../psl-examples/{}/data/{}/{}/{}/{}_targets.txt\".format(dirname, dataset, dataset, fold,\n",
    "                                                                                  phase, predicate)\n",
    "        target_df = pd.read_csv(target_path, sep='\\t', header=None)\n",
    "        \n",
    "\n",
    "    # clean up column names and set multi-index for predicate\n",
    "    arg_columns = ['arg_' + str(col) for col in target_df.columns]\n",
    "    target_df.columns = arg_columns\n",
    "    target_df = target_df.astype({col: int for col in arg_columns})\n",
    "    target_df = target_df.set_index(arg_columns)\n",
    "\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prediction_frame(dataset, wl_method, evaluation_metric, fold, predicate):\n",
    "    # path to this file relative to caller\n",
    "    #     dirname = os.path.dirname(__file__)\n",
    "    dirname='./psl_scripts'\n",
    "\n",
    "    # predicted dataframe\n",
    "    predicted_path = \"{}/../../results/weightlearning/psl/performance_study/{}/{}/{}/{}/inferred-predicates/{}.txt\".format(\n",
    "        dirname, dataset, wl_method, evaluation_metric, fold, predicate.upper())\n",
    "    predicted_df = pd.read_csv(predicted_path, sep='\\t', header=None)\n",
    "\n",
    "    # clean up column names and set multi-index for predicate\n",
    "    arg_columns = ['arg_' + str(col) for col in predicted_df.columns[:-1]]\n",
    "    value_column = ['val']\n",
    "    predicted_df.columns = arg_columns + value_column\n",
    "    predicted_df = predicted_df.astype({col: int for col in arg_columns})\n",
    "    predicted_df = predicted_df.set_index(arg_columns)\n",
    "\n",
    "    return predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "842"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df = load_target_frame(dataset, fold, predicate, phase='eval')\n",
    "observed_df = load_observed_frame(dataset, fold, predicate, phase='eval')\n",
    "truth_df = load_truth_frame(dataset, fold, predicate, phase='eval')\n",
    "\n",
    "truth_df.index.isin(observed_df.index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mse(predicted_df, truth_df, observed_df, target_df):\n",
    "    # consider overlap between observed and truths if there is observed truths\n",
    "    complete_predictions = observed_df.append(predicted_df)\n",
    "    complete_predictions = complete_predictions.loc[~complete_predictions.index.duplicated(keep='first')]\n",
    "    \n",
    "    # evaluator indices\n",
    "    evaluator_indices = truth_df.index.intersection(target_df.index)\n",
    "\n",
    "    # Join predicted_df and truth_df on the arguments \n",
    "    experiment_frame = truth_df.loc[evaluator_indices].join(complete_predictions, how=\"left\",\n",
    "                                                            lsuffix='_truth', rsuffix='_predicted')\n",
    "    \n",
    "    return mean_squared_error(experiment_frame.val_truth, experiment_frame.val_predicted)\n",
    "\n",
    "\n",
    "def evaluate_accuracy(predicted_df, truth_df, observed_df, target_df):\n",
    "    # consider overlap between observed and truths if there is observed truths\n",
    "    complete_predictions = observed_df.append(predicted_df)\n",
    "    complete_predictions = complete_predictions.loc[~complete_predictions.index.duplicated(keep='first')]\n",
    "\n",
    "    # use the category with the highest value as prediction, subset by target index\n",
    "    predicted_categories_df = complete_predictions.loc[target_df.index].groupby(level=0).transform(\n",
    "        lambda x: x.index.isin(x.iloc[[x.argmax()]].index))\n",
    "    \n",
    "    # boolean for truth df type\n",
    "    truth_df = (truth_df == 1)\n",
    "    \n",
    "    # Join predicted_df and truth_df on the arguments\n",
    "    # By right joining and filling with False we are closing the truth since the\n",
    "    # predicted_categories_df should have all targets and the truth frame may only have the positives\n",
    "    experiment_frame = truth_df.join(predicted_categories_df, how=\"right\",\n",
    "                                     lsuffix='_truth', rsuffix='_predicted').fillna(False)\n",
    "    \n",
    "    return accuracy_score(experiment_frame.val_truth, experiment_frame.val_predicted)\n",
    "\n",
    "\n",
    "def evaluate_f1(predicted_df, truth_df, observed_df, target_df, threshold=0.5):\n",
    "    # consider overlap between observed and truths if there is observed truths\n",
    "    complete_predictions = observed_df.append(predicted_df)\n",
    "    complete_predictions = complete_predictions.loc[~complete_predictions.index.duplicated(keep='first')]\n",
    "    \n",
    "    # use the category with the highest value as prediction, subset by target index\n",
    "    predicted_categories_df = complete_predictions.loc[target_df.index].groupby(level=0).transform(\n",
    "        lambda x: x.index.isin(x.iloc[[x.argmax()]].index))\n",
    "    \n",
    "    # boolean for truth df type\n",
    "    truth_df = (truth_df == 1)\n",
    "    \n",
    "    # By right joining and filling with 0 we are closing the truth since the\n",
    "    # complete_predictions.loc[target_df.index] should have all targets and the\n",
    "    # truth frame may only have the positives\n",
    "    experiment_frame = truth_df.join(predicted_categories_df, how=\"right\",\n",
    "                                     lsuffix='_truth', rsuffix='_predicted').fillna(False)\n",
    "    \n",
    "    return f1_score(experiment_frame.val_truth, experiment_frame.val_predicted, pos_label=True)\n",
    "\n",
    "\n",
    "def evaluate_roc_auc_score(predicted_df, truth_df, observed_df, target_df, threshold=0.5):\n",
    "    # consider overlap between observed and truths if there is observed truths\n",
    "    complete_predictions = observed_df.append(predicted_df)\n",
    "    complete_predictions = complete_predictions.loc[~complete_predictions.index.duplicated(keep='first')]\n",
    "    \n",
    "    # evaluator indices\n",
    "    evaluator_indices = truth_df.index.intersection(target_df.index)\n",
    "\n",
    "    # Join predicted_df and truth_df on the arguments\n",
    "    # By right joining and filling with 0 we are closing the truth since the\n",
    "    # complete_predictions.loc[target_df.index] should have all targets and the\n",
    "    # truth frame may only have the positives\n",
    "    experiment_frame = truth_df.loc[evaluator_indices].join(complete_predictions.loc[target_df.index], how=\"right\",\n",
    "                                     lsuffix='_truth', rsuffix='_predicted').fillna(0)\n",
    "\n",
    "    relevant = experiment_frame.val_truth > threshold\n",
    "    return roc_auc_score(relevant, experiment_frame.val_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_name_to_method = {\n",
    "    'Categorical': evaluate_accuracy,\n",
    "    'Discrete': evaluate_f1,\n",
    "    'Continuous': evaluate_mse,\n",
    "    'Ranking': evaluate_roc_auc_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.829953488372093"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df=load_prediction_frame(dataset, wl_method, evaluation_metric, fold, predicate)\n",
    "truth_df=load_truth_frame(dataset, fold, predicate)\n",
    "observed_df=load_observed_frame(dataset, fold, predicate)\n",
    "target_df=load_target_frame(dataset, fold, predicate)\n",
    "evaluate_accuracy(predicted_df, truth_df, observed_df, target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([(2374, 4),\n",
       "            (2807, 2),\n",
       "            ( 527, 1),\n",
       "            (1932, 5),\n",
       "            (3029, 4),\n",
       "            (2092, 4),\n",
       "            (2340, 3),\n",
       "            ( 649, 4),\n",
       "            (2183, 2),\n",
       "            (2947, 3),\n",
       "            ...\n",
       "            (1739, 5),\n",
       "            (2468, 5),\n",
       "            (2542, 4),\n",
       "            (3280, 3),\n",
       "            (1874, 5),\n",
       "            (1558, 5),\n",
       "            (2924, 5),\n",
       "            (1705, 6),\n",
       "            (1792, 2),\n",
       "            ( 165, 3)],\n",
       "           names=['arg_0', 'arg_1'], length=814)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_df.index.intersection(target_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_performance_results(method):\n",
    "    # in results/weightlearning/{}/performance_study write \n",
    "    # a performance.csv file with columns \n",
    "    # Dataset | WL_Method | Evaluation_Method | Mean | Standard_Deviation\n",
    "    \n",
    "    # we are going to overwrite the file with all the most up to date information\n",
    "    \n",
    "    performance_frame = pd.DataFrame(columns=['Dataset', 'Wl_Method', 'Evaluation_Method', 'Mean', 'Standard_Deviation'])\n",
    "    \n",
    "    # extract all the files that are in the results directory\n",
    "    path = '../results/weightlearning/{}/performance_study'.format(method)\n",
    "    datasets = [dataset for dataset in os.listdir(path) if os.path.isdir(os.path.join(path, dataset))]\n",
    "    \n",
    "    # iterate over all datasets adding the results to the performance_frame\n",
    "    for dataset in datasets:\n",
    "        # extract all the wl_methods that are in the directory\n",
    "        path = '../results/weightlearning/{}/performance_study/{}'.format(method, dataset)\n",
    "        wl_methods = [wl_method for wl_method in os.listdir(path) if os.path.isdir(os.path.join(path, wl_method))]\n",
    "        \n",
    "        for wl_method in wl_methods:\n",
    "            # extract all the metrics that are in the directory\n",
    "            path = '../results/weightlearning/{}/performance_study/{}/{}'.format(method, dataset, wl_method)\n",
    "            evaluators = [evaluator for evaluator in os.listdir(path) if os.path.isdir(os.path.join(path, evaluator))]\n",
    "\n",
    "            for evaluator in evaluators:\n",
    "                # extract all the folds that are in the directory\n",
    "                path = '../results/weightlearning/{}/performance_study/{}/{}/{}'.format(method, dataset, wl_method, evaluator)\n",
    "                folds = [fold for fold in os.listdir(path) if os.path.isdir(os.path.join(path, fold))]\n",
    "                \n",
    "                # initialize the experiment list that will be populated in the following for \n",
    "                # loop with the performance outcome of each fold\n",
    "                experiment_performance = np.array([])\n",
    "\n",
    "                for fold in folds:\n",
    "                    # load the prediction dataframe\n",
    "                    try:\n",
    "                        # prediction dataframe\n",
    "                        if method == 'psl':\n",
    "                            predicted_df = load_psl_prediction_frame(dataset, wl_method, evaluator, fold, \n",
    "                                                                     dataset_properties[dataset]['evaluation_predicate'])\n",
    "                        elif method == 'tuffy':\n",
    "                            predicted_df = load_tuffy_prediction_frame(dataset, wl_method, evaluator, fold, \n",
    "                                                                       dataset_properties[dataset]['evaluation_predicate'])\n",
    "                        else:\n",
    "                            raise IllegalArgumentError(\"{} not supported. Try: ['psl', 'tuffy']\".format(method))\n",
    "                    except FileNotFoundError as err:\n",
    "                        print(err)\n",
    "                        continue\n",
    "                    \n",
    "                    # truth dataframe \n",
    "                    truth_df = load_truth_frame(dataset, fold, dataset_properties[dataset]['evaluation_predicate'])\n",
    "                    # observed dataframe\n",
    "                    observed_df = load_observed_frame(dataset, fold, dataset_properties[dataset]['evaluation_predicate'])\n",
    "                    \n",
    "                    experiment_performance = np.append(experiment_performance, \n",
    "                                                       evaluator_name_to_method[evaluator](predicted_df, truth_df, observed_df))\n",
    "    \n",
    "                # update the performance_frame\n",
    "                performance_series = pd.Series(index=['Dataset', 'Wl_Method', 'Evaluation_Method', 'Mean', 'Standard_Deviation'],\n",
    "                                               dtype=float)\n",
    "                performance_series['Dataset'] = dataset\n",
    "                performance_series['Wl_Method'] = wl_method\n",
    "                performance_series['Evaluation_Method'] = evaluator\n",
    "                performance_series['Mean'] = experiment_performance.mean()\n",
    "                performance_series['Standard_Deviation'] = experiment_performance.std()\n",
    "                \n",
    "                performance_frame = performance_frame.append(performance_series, ignore_index=True)\n",
    "                \n",
    "    # write results frame to results/weightlearning/{}/performance_study\n",
    "    #performance_frame.to_csv('../results/weightlearning/{}/performance_study/{}_performance.csv'.format(method, method), index=False)\n",
    "    return performance_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the percent increase for each dataset and evaluator\n",
    "performance_results = pd.read_csv(\"../results/weightlearning/psl/performance_study/psl_performance.csv\")\n",
    "performance_results['PCT_Increase'] = 0\n",
    "for dataset in performance_results.Dataset.unique():\n",
    "    dataset_performance = performance_results[performance_results.Dataset == dataset]\n",
    "    for Evaluation_Method in dataset_performance.Evaluation_Method.unique():\n",
    "        evaluator_performance = dataset_performance[dataset_performance.Evaluation_Method == Evaluation_Method]\n",
    "        Uniform_performance = evaluator_performance[evaluator_performance.Wl_Method == \"UNIFORM\"].Mean.values[0]\n",
    "        pct_increase = ((evaluator_performance.Mean - Uniform_performance) / Uniform_performance) * 100\n",
    "        performance_results.loc[evaluator_performance.index, \"PCT_Improved\"] = pct_increase\n",
    "\n",
    "performance_results.to_csv('../results/weightlearning/psl/performance_study/psl_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_timing_results(method):\n",
    "    # In results/weightlearning/psl/performance_study write a {}_timing.csv file with columns \n",
    "    # WL_Method | Dataset | Evaluation_Method | Mean_User_Time | Mean_Sys_Time | \n",
    "    \n",
    "    # we are going to overwrite the file with all the most up to date information\n",
    "    timing_frame = pd.DataFrame(columns=['Dataset', 'Wl_Method', 'Evaluation_Method', 'Mean_User_Time', \n",
    "                                         'Mean_Sys_Time', 'Sys_Time_Standard_Deviation', 'User_Time_Standard_Deviation'])\n",
    "    \n",
    "    # extract all the files that are in the results directory\n",
    "    path = '../results/weightlearning/{}/performance_study'.format(method)\n",
    "    datasets = [dataset for dataset in os.listdir(path) if os.path.isdir(os.path.join(path, dataset))]\n",
    "    \n",
    "    # iterate over all datasets adding the results to the timing_frame\n",
    "    for dataset in datasets:\n",
    "        # extract all the wl_methods that are in the directory\n",
    "        path = '../results/weightlearning/{}/performance_study/{}'.format(method, dataset)\n",
    "        wl_methods = [wl_method for wl_method in os.listdir(path) if os.path.isdir(os.path.join(path, wl_method))]\n",
    "        \n",
    "        for wl_method in wl_methods:\n",
    "            # extract all the metrics that are in the directory\n",
    "            path = '../results/weightlearning/{}/performance_study/{}/{}'.format(method, dataset, wl_method)\n",
    "            evaluators = [evaluator for evaluator in os.listdir(path) if os.path.isdir(os.path.join(path, evaluator))]\n",
    "\n",
    "            for evaluator in evaluators:\n",
    "                # extract all the folds that are in the directory\n",
    "                path = '../results/weightlearning/{}/performance_study/{}/{}/{}'.format(method, dataset, wl_method, evaluator)\n",
    "                folds = [fold for fold in os.listdir(path) if os.path.isdir(os.path.join(path, fold))]\n",
    "                \n",
    "                # initialize the experiment_timing_frame that will be populated in the following for loop\n",
    "                experiment_timing_frame = pd.DataFrame(columns=['User time (seconds)', 'System time (seconds)'])\n",
    "\n",
    "                for fold in folds:\n",
    "                    path = '../results/weightlearning/{}/performance_study/{}/{}/{}/{}'.format(\n",
    "                        method, dataset, wl_method, evaluator, fold\n",
    "                    )\n",
    "                    # load the prediction dataframe\n",
    "                    try:\n",
    "                        # timing series for fold\n",
    "                        fold_timing_series = pd.read_csv(path + '/time.txt', sep=': ', engine='python', header=None, index_col=0)\n",
    "                        # add fold timing to experiment timing\n",
    "                        experiment_timing_frame = experiment_timing_frame.append(\n",
    "                            fold_timing_series.loc[['User time (seconds)', 'System time (seconds)'], 1],\n",
    "                            ignore_index=True\n",
    "                        )\n",
    "                    except (FileNotFoundError, pd.errors.EmptyDataError, KeyError) as err:\n",
    "                        print('{}: {}'.format(path, err))\n",
    "                        continue\n",
    "    \n",
    "                # parse the timing series\n",
    "                timing_series = pd.Series(index=['Dataset', 'Wl_Method', 'Mean_User_Time', 'Mean_Sys_Time', \n",
    "                                                 'User_Time_Standard_Deviation', 'Sys_Time_Standard_Deviation'],\n",
    "                                               dtype=float)\n",
    "                experiment_timing_frame = experiment_timing_frame.astype({'User time (seconds)': float,\n",
    "                                                                          'System time (seconds)': float})\n",
    "                timing_series['Dataset'] = dataset\n",
    "                timing_series['Wl_Method'] = wl_method\n",
    "                timing_series['Mean_User_Time'] = experiment_timing_frame['User time (seconds)'].mean()\n",
    "                timing_series['Mean_Sys_Time'] = experiment_timing_frame['System time (seconds)'].mean()\n",
    "                timing_series['User_Time_Standard_Deviation'] = experiment_timing_frame['User time (seconds)'].std()\n",
    "                timing_series['Sys_Time_Standard_Deviation'] = experiment_timing_frame['System time (seconds)'].std()\n",
    "                timing_frame = timing_frame.append(timing_series, ignore_index=True)\n",
    "                \n",
    "    # write results frame to results/weightlearning/{}/performance_study\n",
    "    timing_frame.to_csv('../results/weightlearning/{}/performance_study/{}_timing.csv'.format(method, method), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/weightlearning/psl/performance_study/lastfm/RGS/Continuous/1: No columns to parse from file\n",
      "../results/weightlearning/psl/performance_study/epinions/HB/Ranking/6: 1\n"
     ]
    }
   ],
   "source": [
    "write_timing_results('psl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = load_psl_prediction_frame(dataset, wl_method, evaluation_metric, 0, dataset_properties[dataset]['evaluation_predicate'])\n",
    "truth_df = load_truth_frame(dataset, 0, dataset_properties[dataset]['evaluation_predicate'])\n",
    "observed_df = load_observed_frame(dataset, 0, dataset_properties[dataset]['evaluation_predicate'])\n",
    "targets_df = load_target_frame(dataset, 0, dataset_properties[dataset]['evaluation_predicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7774086378737541"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join predicted_df and truth_df on the arguments \n",
    "threshold = 0.5\n",
    "complete_predictions = observed_df.append(predicted_df)\n",
    "experiment_frame = truth_df.join(complete_predictions, how=\"left\", \n",
    "                                 lsuffix='_truth', rsuffix='_predicted').fillna(0)\n",
    "\n",
    "rounded_predictions = experiment_frame.val_predicted > threshold\n",
    "rounded_truths = experiment_frame.val_truth > threshold\n",
    "\n",
    "f1_score(rounded_truths, rounded_predictions, pos_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1656, 1)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] File ../results/weightlearning/psl/performance_study/cora/BOWLSS/Categorical/2/inferred-predicates/HASCAT.txt does not exist: '../results/weightlearning/psl/performance_study/cora/BOWLSS/Categorical/2/inferred-predicates/HASCAT.txt'\n",
      "     Dataset Wl_Method Evaluation_Method      Mean  Standard_Deviation\n",
      "0       cora    BOWLOS       Categorical  0.956505            0.002855\n",
      "1       cora    BOWLOS          Discrete  0.463495            0.019231\n",
      "2       cora   UNIFORM       Categorical  0.956452            0.003773\n",
      "3       cora   UNIFORM          Discrete  0.367932            0.017214\n",
      "4       cora    BOWLSS       Categorical  0.955041            0.003231\n",
      "5     jester       RGS           Ranking  0.767714            0.003209\n",
      "6     jester       RGS        Continuous  0.053321            0.001119\n",
      "7     jester        HB           Ranking  0.766385            0.001948\n",
      "8     jester        HB        Continuous  0.069226            0.000335\n",
      "9     jester    BOWLOS           Ranking  0.769799            0.001512\n",
      "10    jester    BOWLOS        Continuous  0.053383            0.000920\n",
      "11    jester      CRGS           Ranking  0.766734            0.001818\n",
      "12    jester      CRGS        Continuous  0.067476            0.000886\n",
      "13    jester   UNIFORM           Ranking  0.759833            0.002137\n",
      "14    jester   UNIFORM        Continuous  0.077202            0.000285\n",
      "15    jester    BOWLSS           Ranking  0.767810            0.001571\n",
      "16    jester    BOWLSS        Continuous  0.057269            0.000186\n",
      "17  citeseer       RGS       Categorical  0.925786            0.003034\n",
      "18  citeseer       RGS          Discrete  0.344883            0.010219\n",
      "19  citeseer        HB       Categorical  0.923272            0.003162\n",
      "20  citeseer        HB          Discrete  0.296983            0.010509\n",
      "21  citeseer    BOWLOS       Categorical  0.925638            0.003066\n",
      "22  citeseer    BOWLOS          Discrete  0.346051            0.011164\n",
      "23  citeseer      CRGS       Categorical  0.923741            0.002854\n",
      "24  citeseer      CRGS          Discrete  0.306453            0.009064\n",
      "25  citeseer   UNIFORM       Categorical  0.922188            0.002329\n",
      "26  citeseer   UNIFORM          Discrete  0.274037            0.010517\n",
      "27  citeseer    BOWLSS       Categorical  0.925688            0.002881\n",
      "28  citeseer    BOWLSS          Discrete  0.345098            0.011832\n"
     ]
    }
   ],
   "source": [
    "write_psl_performance_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arg_0</th>\n",
       "      <th>arg_1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1656 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             val\n",
       "arg_0 arg_1     \n",
       "2     4      1.0\n",
       "4     3      1.0\n",
       "5     2      1.0\n",
       "10    6      1.0\n",
       "12    1      1.0\n",
       "...          ...\n",
       "3307  5      1.0\n",
       "3308  3      1.0\n",
       "3309  5      1.0\n",
       "3310  4      1.0\n",
       "3311  3      1.0\n",
       "\n",
       "[1656 rows x 1 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arg_0</th>\n",
       "      <th>arg_1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>1</th>\n",
       "      <td>0.074147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.074147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.079410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.521938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3311</th>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10750 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  val\n",
       "arg_0 arg_1          \n",
       "2     1      0.074147\n",
       "      2      0.074147\n",
       "      3      0.079410\n",
       "      4      0.100749\n",
       "      5      0.521938\n",
       "...               ...\n",
       "3311  2      0.000000\n",
       "      4      0.000000\n",
       "      5      0.000000\n",
       "      6      0.000000\n",
       "      7      0.000000\n",
       "\n",
       "[10750 rows x 1 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build single dataframe for each dataset\n",
    "for dataset_name in datasets.keys():\n",
    "    datasets[dataset_name][\"dataframe\"] = pd.DataFrame(index=range(datasets[dataset_name][\"n_folds\"]));\n",
    "    datasets[dataset_name][\"dataframe\"].index.name = 'fold'\n",
    "    for evaluation_method in datasets[dataset_name][\"evaluation_methods\"]:\n",
    "        for wl_method in weight_learning_methods:\n",
    "            results = pd.read_csv(dataset_name + '/' + dataset_name + \"-\" + wl_method + \"-eval-\" + evaluation_method + \".csv\", header=None).values[0]\n",
    "            results = results[:datasets[dataset_name]['n_folds']]\n",
    "            datasets[dataset_name][\"dataframe\"][evaluation_method + \"_\" + wl_method] = results\n",
    "\n",
    "columns=set()\n",
    "for dataset_name in datasets.keys():\n",
    "    for evaluation_method in datasets[dataset_name][\"evaluation_methods\"]:\n",
    "        columns.add(dataset_name + '_' + evaluation_method + '_mean')\n",
    "        columns.add(dataset_name + '_' + evaluation_method + '_standardDeviation')\n",
    "\n",
    "performance_dataframe = pd.DataFrame(index=weight_learning_methods, columns=columns)\n",
    "for wl_method in weight_learning_methods:\n",
    "    for dataset_name in datasets.keys():\n",
    "        for evaluation_method in datasets[dataset_name][\"evaluation_methods\"]:\n",
    "            results = pd.read_csv(dataset_name + '/' + dataset_name + \"-\" + wl_method + \"-eval-\" + evaluation_method + \".csv\", header=None).values[0]\n",
    "            results = results[:datasets[dataset_name]['n_folds']]\n",
    "            performance_dataframe.loc[wl_method, dataset_name + '_' + evaluation_method + '_mean'] = results.mean()\n",
    "            performance_dataframe.loc[wl_method, dataset_name + '_' + evaluation_method + '_standardDeviation'] = results.std()\n",
    "            \n",
    "performance_dataframe = performance_dataframe.reindex(sorted(performance_dataframe.columns), axis=1)\n",
    "performance_dataframe.to_csv(\"performance_study_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citeseer_Accuracy_mean</th>\n",
       "      <th>Citeseer_Accuracy_standardDeviation</th>\n",
       "      <th>Citeseer_F1_mean</th>\n",
       "      <th>Citeseer_F1_standardDeviation</th>\n",
       "      <th>Cora_Accuracy_mean</th>\n",
       "      <th>Cora_Accuracy_standardDeviation</th>\n",
       "      <th>Cora_F1_mean</th>\n",
       "      <th>Cora_F1_standardDeviation</th>\n",
       "      <th>Epinions_AUROC_mean</th>\n",
       "      <th>Epinions_AUROC_standardDeviation</th>\n",
       "      <th>Epinions_F1_mean</th>\n",
       "      <th>Epinions_F1_standardDeviation</th>\n",
       "      <th>Jester_AUROC_mean</th>\n",
       "      <th>Jester_AUROC_standardDeviation</th>\n",
       "      <th>Jester_MSE_mean</th>\n",
       "      <th>Jester_MSE_standardDeviation</th>\n",
       "      <th>LastFM_AUROC_mean</th>\n",
       "      <th>LastFM_AUROC_standardDeviation</th>\n",
       "      <th>LastFM_MSE_mean</th>\n",
       "      <th>LastFM_MSE_standardDeviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RGS</th>\n",
       "      <td>0.741013</td>\n",
       "      <td>0.0117652</td>\n",
       "      <td>0.797617</td>\n",
       "      <td>0.00695118</td>\n",
       "      <td>0.83299</td>\n",
       "      <td>0.014448</td>\n",
       "      <td>0.866319</td>\n",
       "      <td>0.0148332</td>\n",
       "      <td>0.811908</td>\n",
       "      <td>0.0302242</td>\n",
       "      <td>0.964475</td>\n",
       "      <td>0.00568882</td>\n",
       "      <td>0.769814</td>\n",
       "      <td>0.00155714</td>\n",
       "      <td>0.0521525</td>\n",
       "      <td>0.000476229</td>\n",
       "      <td>0.598016</td>\n",
       "      <td>0.00459492</td>\n",
       "      <td>0.078146</td>\n",
       "      <td>0.000922336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRGS</th>\n",
       "      <td>0.744561</td>\n",
       "      <td>0.0106014</td>\n",
       "      <td>0.788449</td>\n",
       "      <td>0.00798026</td>\n",
       "      <td>0.833914</td>\n",
       "      <td>0.0176771</td>\n",
       "      <td>0.855506</td>\n",
       "      <td>0.0115167</td>\n",
       "      <td>0.814475</td>\n",
       "      <td>0.0217344</td>\n",
       "      <td>0.957941</td>\n",
       "      <td>0.00634273</td>\n",
       "      <td>0.771078</td>\n",
       "      <td>0.00151763</td>\n",
       "      <td>0.0552375</td>\n",
       "      <td>0.000269525</td>\n",
       "      <td>0.594382</td>\n",
       "      <td>0.0018314</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>0.000832697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HB</th>\n",
       "      <td>0.743052</td>\n",
       "      <td>0.00880311</td>\n",
       "      <td>0.785321</td>\n",
       "      <td>0.00770918</td>\n",
       "      <td>0.836037</td>\n",
       "      <td>0.0152339</td>\n",
       "      <td>0.857889</td>\n",
       "      <td>0.0099019</td>\n",
       "      <td>0.808821</td>\n",
       "      <td>0.0224004</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.00581896</td>\n",
       "      <td>0.7709</td>\n",
       "      <td>0.00151919</td>\n",
       "      <td>0.0571412</td>\n",
       "      <td>0.000176028</td>\n",
       "      <td>0.594382</td>\n",
       "      <td>0.0018314</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>0.000832697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Citeseer_Accuracy_mean Citeseer_Accuracy_standardDeviation  \\\n",
       "RGS                0.741013                           0.0117652   \n",
       "CRGS               0.744561                           0.0106014   \n",
       "HB                 0.743052                          0.00880311   \n",
       "\n",
       "     Citeseer_F1_mean Citeseer_F1_standardDeviation Cora_Accuracy_mean  \\\n",
       "RGS          0.797617                    0.00695118            0.83299   \n",
       "CRGS         0.788449                    0.00798026           0.833914   \n",
       "HB           0.785321                    0.00770918           0.836037   \n",
       "\n",
       "     Cora_Accuracy_standardDeviation Cora_F1_mean Cora_F1_standardDeviation  \\\n",
       "RGS                         0.014448     0.866319                 0.0148332   \n",
       "CRGS                       0.0176771     0.855506                 0.0115167   \n",
       "HB                         0.0152339     0.857889                 0.0099019   \n",
       "\n",
       "     Epinions_AUROC_mean Epinions_AUROC_standardDeviation Epinions_F1_mean  \\\n",
       "RGS             0.811908                        0.0302242         0.964475   \n",
       "CRGS            0.814475                        0.0217344         0.957941   \n",
       "HB              0.808821                        0.0224004         0.957346   \n",
       "\n",
       "     Epinions_F1_standardDeviation Jester_AUROC_mean  \\\n",
       "RGS                     0.00568882          0.769814   \n",
       "CRGS                    0.00634273          0.771078   \n",
       "HB                      0.00581896            0.7709   \n",
       "\n",
       "     Jester_AUROC_standardDeviation Jester_MSE_mean  \\\n",
       "RGS                      0.00155714       0.0521525   \n",
       "CRGS                     0.00151763       0.0552375   \n",
       "HB                       0.00151919       0.0571412   \n",
       "\n",
       "     Jester_MSE_standardDeviation LastFM_AUROC_mean  \\\n",
       "RGS                   0.000476229          0.598016   \n",
       "CRGS                  0.000269525          0.594382   \n",
       "HB                    0.000176028          0.594382   \n",
       "\n",
       "     LastFM_AUROC_standardDeviation LastFM_MSE_mean  \\\n",
       "RGS                      0.00459492        0.078146   \n",
       "CRGS                      0.0018314        0.091716   \n",
       "HB                        0.0018314        0.091716   \n",
       "\n",
       "     LastFM_MSE_standardDeviation  \n",
       "RGS                   0.000922336  \n",
       "CRGS                  0.000832697  \n",
       "HB                    0.000832697  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets.keys():\n",
    "    datasets[dataset_name][\"dataframe\"] = pd.DataFrame(index=range(datasets[dataset_name][\"n_folds\"]));\n",
    "    datasets[dataset_name][\"dataframe\"].index.name = 'fold'\n",
    "    for evaluation_method in datasets[dataset_name][\"evaluation_methods\"]:\n",
    "        for wl_method in weight_learning_methods:\n",
    "            results = pd.read_csv(dataset_name + '/' + dataset_name + \"-\" + wl_method + \"-eval-\" + evaluation_method + \".csv\", header=None).values[0]\n",
    "            results = results[:datasets[dataset_name]['n_folds']]\n",
    "            datasets[dataset_name][\"dataframe\"][evaluation_method + \"_\" + wl_method] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcc001aecc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'Jester'\n",
    "datasets[dataset_name][\"dataframe\"].mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=set()\n",
    "for dataset_name in datasets.keys():\n",
    "    for column in datasets[dataset_name]['dataframe'].columns:\n",
    "        columns.add(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dataframe = pd.DataFrame(columns=columns)\n",
    "standard_deviation_dataframe = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets.keys():\n",
    "    mean_series = datasets[dataset_name]['dataframe'].mean()\n",
    "    mean_series.name = dataset_name\n",
    "    mean_dataframe = mean_dataframe.append(mean_series)\n",
    "    \n",
    "    standard_deviation_series = datasets[dataset_name]['dataframe'].std()\n",
    "    standard_deviation_series.name = dataset_name\n",
    "    standard_deviation_dataframe = standard_deviation_dataframe.append(standard_deviation_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dataframe = standard_deviation_dataframe.join(mean_dataframe, lsuffix='_std', rsuffix='_mean')\n",
    "performance_dataframe = performance_dataframe.reindex(sorted(performance_dataframe.columns), axis=1)\n",
    "performance_dataframe.to_csv(\"performance_study_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RGS_MSE</th>\n",
       "      <th>CRGS_F1</th>\n",
       "      <th>HB_MSE</th>\n",
       "      <th>HB_F1</th>\n",
       "      <th>RGS_F1</th>\n",
       "      <th>RGS_Accuracy</th>\n",
       "      <th>HB_Accuracy</th>\n",
       "      <th>CRGS_MSE</th>\n",
       "      <th>CRGS_Accuracy</th>\n",
       "      <th>RGS_AUROC</th>\n",
       "      <th>HB_AUROC</th>\n",
       "      <th>CRGS_AUROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jester</th>\n",
       "      <td>0.052152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.769814</td>\n",
       "      <td>0.770900</td>\n",
       "      <td>0.771078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epinions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.964475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.811908</td>\n",
       "      <td>0.808821</td>\n",
       "      <td>0.814475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cora</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857889</td>\n",
       "      <td>0.866319</td>\n",
       "      <td>0.832990</td>\n",
       "      <td>0.836037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Citeseer</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.788449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.785321</td>\n",
       "      <td>0.797617</td>\n",
       "      <td>0.741013</td>\n",
       "      <td>0.743053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.744561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastFM</th>\n",
       "      <td>0.078146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598016</td>\n",
       "      <td>0.594382</td>\n",
       "      <td>0.594382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RGS_MSE   CRGS_F1    HB_MSE     HB_F1    RGS_F1  RGS_Accuracy  \\\n",
       "Jester    0.052152       NaN  0.057141       NaN       NaN           NaN   \n",
       "Epinions       NaN  0.957941       NaN  0.957346  0.964475           NaN   \n",
       "Cora           NaN  0.855506       NaN  0.857889  0.866319      0.832990   \n",
       "Citeseer       NaN  0.788449       NaN  0.785321  0.797617      0.741013   \n",
       "LastFM    0.078146       NaN  0.091716       NaN       NaN           NaN   \n",
       "\n",
       "          HB_Accuracy  CRGS_MSE  CRGS_Accuracy  RGS_AUROC  HB_AUROC  \\\n",
       "Jester            NaN  0.055238            NaN   0.769814  0.770900   \n",
       "Epinions          NaN       NaN            NaN   0.811908  0.808821   \n",
       "Cora         0.836037       NaN       0.833914        NaN       NaN   \n",
       "Citeseer     0.743053       NaN       0.744561        NaN       NaN   \n",
       "LastFM            NaN  0.091716            NaN   0.598016  0.594382   \n",
       "\n",
       "          CRGS_AUROC  \n",
       "Jester      0.771078  \n",
       "Epinions    0.814475  \n",
       "Cora             NaN  \n",
       "Citeseer         NaN  \n",
       "LastFM      0.594382  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1', '2'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[0,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f72541674636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "l.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
